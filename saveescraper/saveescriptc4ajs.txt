#!/usr/bin/env python3
"""
Generated by Crawl4AI Chrome Extension - Script Builder (ALPHA)
URL: https://savee.com/i/NUjvXox/
Generated: 2025-08-20T11:45:05.500Z
"""

import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

# JavaScript code to execute
JS_SCRIPT = """
document.querySelector('div:nth-child(1) > main > div:nth-child(5) > div:nth-child(1) > div > div:nth-child(1) > div > div:nth-child(3) > div > a > div:nth-child(2) > div:nth-child(2) > div').click();\ndocument.querySelector('div:nth-child(13) > div > div > div > div:nth-child(2) > div > div > div:nth-child(1) > div > div:nth-child(1) > ul > li:nth-child(4) > ul > button > span:nth-child(1)').click();\n// Scroll element
const scrollEl = document.querySelector('div:nth-child(13) > div > div > div > div:nth-child(2) > div > div');
if (scrollEl) {
  scrollEl.scrollTop += NaN;
}
"""

async def run_automation():
    """Run the recorded automation script"""
    
    # Configure browser
    browser_config = BrowserConfig(
        headless=False,  # Set to True for headless mode
        verbose=True
    )
    
    # Configure crawler with JavaScript execution
    crawler_config = CrawlerRunConfig(
        js_code=JS_SCRIPT,
        wait_for="js:() => document.readyState === 'complete'",
        page_timeout=30000  # 30 seconds timeout
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://savee.com/i/NUjvXox/",
            config=crawler_config
        )
        
        if result.success:
            print("✅ Automation completed successfully!")
            print(f"Final URL: {result.url}")
            # You can access the final HTML with result.html
            # Or extracted content with result.cleaned_html
        else:
            print("❌ Automation failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(run_automation())
